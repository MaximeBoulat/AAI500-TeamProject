{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Datasets/Credit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset -- Skip first metadata row\n",
    "df = pd.read_csv('Datasets/Credit.csv')\n",
    "\n",
    "# Drop the ID column as it's not useful for analysis\n",
    "df_cleaned = df.drop(columns=[\"ID\"])\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Generating Correlation Heatmap\n",
    "plt.figure(figsize=(18, 14))\n",
    "correlation_matrix = df_cleaned.corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Printing my explanation\n",
    "print(\"explanation\")\n",
    "print()\n",
    "print(\"The correlation heatmap of the dataset reveals relationships between features such as: High correlation among BILL_AMT variables (e.g., BILL_AMT1, BILL_AMT2, etc.)\")\n",
    "print(\"The correlation heatmap also indicates a positive correlation between LIMIT_BAL and PAY_AMT values, a Mmoderate positive correlation between past payment statuses (PAY_0 to PAY_6) and default likelihood\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset (assuming the file is in the same directory) --Skip first metadata row\n",
    "df = pd.read_csv('Datasets/Credit.csv')\n",
    "\n",
    "# Drop the ID column as it's not useful for analysis\n",
    "df_cleaned = df.drop(columns=[\"ID\"])\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# --- Histogram of Credit Limit ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_cleaned[\"LIMIT_BAL\"], bins=30, kde=True, color=\"skyblue\")\n",
    "plt.title(\"Distribution of Credit Limit (LIMIT_BAL)\")\n",
    "plt.xlabel(\"Credit Limit\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Printing my explanation\n",
    "print(\"Explanation -- The histogram of the credit limit (LIMIT_BAL)\")\n",
    "print()\n",
    "print(\"Most credit limits are concentrated below 200,000 units.\")\n",
    "print(\"The distribution is right-skewed, indicating a smaller number of clients with very high credit limits.\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('Datasets/Credit.csv')\n",
    "df = df.drop(columns=[\"ID\"])\n",
    "\n",
    "# Create AGE groups\n",
    "age_bins = [20, 30, 40, 50, 60, 70, 80]\n",
    "age_labels = ['20s', '30s', '40s', '50s', '60s', '70s']\n",
    "df['AGE_GROUP'] = pd.cut(df['AGE'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "# Define population segments\n",
    "segment_columns = ['SEX', 'EDUCATION', 'MARRIAGE', 'AGE_GROUP']\n",
    "segment_group = df.groupby(segment_columns)\n",
    "\n",
    "# Count total and on-time payments per segment\n",
    "segment_stats = segment_group['default payment next month'].agg(\n",
    "    total='count',\n",
    "    on_time=lambda x: (x == 0).sum()\n",
    ").reset_index()\n",
    "\n",
    "# Calculate P(On-time Segment)\n",
    "segment_stats['P_on_time_given_segment'] = segment_stats['on_time'] / segment_stats['total']\n",
    "\n",
    "# Plot histogram with KDE\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(segment_stats['P_on_time_given_segment'], bins=20, kde=True, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Add vertical lines\n",
    "mean_prob = segment_stats['P_on_time_given_segment'].mean()\n",
    "max_prob = segment_stats['P_on_time_given_segment'].max()\n",
    "\n",
    "plt.axvline(mean_prob, color='orange', linestyle='--', linewidth=2, label=f'Mean: {mean_prob:.2f}')\n",
    "plt.axvline(max_prob, color='green', linestyle='-', linewidth=2, label=f'Max: {max_prob:.2f}')\n",
    "\n",
    "# Labels and legend\n",
    "plt.title(\"Distribution of P(On-time Payment Segment) with KDE\")\n",
    "plt.xlabel(\"P(On-time Payment Segment)\")\n",
    "plt.ylabel(\"Number of Segments\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Printing my explanation\n",
    "print(\"Explanation: I analyzed the probability distribution of a client being granted a credit loan based on their credibility. While the dataset doesn't explicitly say if a client is -granted-- a loan, we can infer credibility based on the target variable\")\n",
    "print()\n",
    "print(\"Plan:Define Credibility: Clients with default payment next month == 0. Analyze how credibility varies with key features (e.g., LIMIT_BAL, PAY_0, AGE, EDUCATION, etc.).Estimate probability:P(Credible Feature) Visualize the probability distribution using a plot (e.g., KDE + Histogram)\")\n",
    "print(\"This Chart Shows: The likelihood of being a credible client (on-time payer) based on credit limit. It looks like creditworthiness (credibility) shifts with the size of the loan\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('Datasets/Credit.csv')\n",
    "\n",
    "# Strip any whitespace from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Rename columns for clarity\n",
    "df.columns = ['ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE',\n",
    "              'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6',\n",
    "              'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "              'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'default']\n",
    "\n",
    "# Clean AGE column and create AGE_GROUP\n",
    "df['AGE'] = pd.to_numeric(df['AGE'], errors='coerce')\n",
    "df = df.dropna(subset=['AGE'])\n",
    "\n",
    "age_bins = [20, 30, 40, 50, 60, 70, 80]\n",
    "age_labels = ['21â€“30', '31â€“40', '41â€“50', '51â€“60', '61â€“70', '71â€“80']\n",
    "df['AGE_GROUP'] = pd.cut(df['AGE'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# Generating the Plot default rates\n",
    "for col in ['EDUCATION', 'MARRIAGE', 'SEX', 'AGE_GROUP']:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    df.groupby(col)['default'].mean().plot(kind='bar', color='skyblue')\n",
    "    plt.title(f'Default Rate by {col}')\n",
    "    plt.ylabel('Default Rate')\n",
    "    plt.xlabel(col)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define feature list\n",
    "features = ['LIMIT_BAL', 'AGE', 'SEX', 'EDUCATION', 'MARRIAGE',\n",
    "            'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6',\n",
    "            'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "            'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "\n",
    "# Preparing features and target\n",
    "X = df[features]\n",
    "y = df['default']\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X = pd.get_dummies(X, columns=['SEX', 'EDUCATION', 'MARRIAGE'], drop_first=True)\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Training Naive Bayes classifier\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Optional: Show sample predictions\n",
    "sample = pd.DataFrame({\n",
    "    'Actual': y_test.values[:5],\n",
    "    'Predicted Probability': y_proba[:5]\n",
    "})\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(sample)\n",
    "\n",
    "#Printing my explanation of the result-set based on the Naive Bayes classifier\n",
    "\n",
    "print(\"Accuracy is 0.377888 -- This means 38%  of the customers were correctly classified â€” either as likely to default (1) or not (0).\")\n",
    "print()\n",
    "print(\"The report breaks down precision, recall, and F1-score for each class\")\n",
    "print()\n",
    "print(\"For Class 0 -- No Default\")\n",
    "print(\"Precision = 0.88: 88% of those predicted as -- No Default were correct\")\n",
    "print(\"Recall = 0.24: 24% of the actual -- no default customers correctly predicted.\")\n",
    "print(\"F1 = 0.37 -- Weak ability to detect actual non-defaulters.\")\n",
    "print()\n",
    "print(\"For Class 1 -- Default\")\n",
    "print(\"Precision = 0.24: 24% of predicted defaulters were actually defaulters\")\n",
    "print(\"Recall = 0.88: 88% of actual defaulters -- Postive case of how many prdicted to be defaulted\")\n",
    "print(\"F1 = 0.38: Weak ability to detect actual defaulter\")\n",
    "print(\" Tha model is too conservative â€” reluctant to label someone as a defaulter.\")\n",
    "print(\"For credit risk, recall on Class 1 is critical â€” you want to catch as many defaulters as possible!\")\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\" --- Sample Predictions ---\")\n",
    "print(\"Actual: The true class -- 0 = no default, 1 = default\")\n",
    "print(\"Predicted Probability: Modelâ€™s confidence that the customer will default\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Row 0: True label is 0 (no default), model predicts 87% chance of default â€” âœ… correct and confident.\")\n",
    "print(\"Row 4: True label is 1 (default), model predicts 87% â€” ðŸŸ¨ somewhat confident, borderline.\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\" --- Recommendations --- \")\n",
    "print(\"Improve recall on defaulters: Try different models like (e.g., logistic regression, random forest), oversampling (SMOTE), or cost-sensitive learning.\")\n",
    "print(\"Threshold tuning: Adjust default classification threshold (not just 0.5) to balance precision/recall.\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
